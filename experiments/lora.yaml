# LoRA-tuned DINO with NeRF Configuration
# This configuration reproduces the LoRA experiment from the paper

experiment:
  name: "lora_nerf_dino"
  description: "NeRF with LoRA-tuned DINO features"
  version: "1.0"

data:
  dataset: "nerf_synthetic"
  scene: "lego"
  num_views: 5
  resolution: [128, 128]
  near: 2.0
  far: 6.0

model:
  # NeRF MLP configuration
  pos_freq: 12
  dir_freq: 4
  hidden_dim: 256
  num_layers: 8
  
  # DINO configuration
  dino_model: "facebook/dinov2-base"
  dino_dim: 64
  feature_projection: "linear"
  
  # LoRA configuration (enabled)
  use_lora: true
  lora_rank: 16
  lora_alpha: 16

training:
  epochs: 200
  learning_rate: 2e-4
  weight_decay: 1e-6
  batch_size: 512
  
  # Progressive training schedule
  progressive_schedule:
    epochs_0_50: [32, 32, 32]  # [H, W, samples_per_ray]
    epochs_50_100: [64, 64, 48]
    epochs_100_plus: [128, 128, 64]
  
  # Loss weights
  rgb_weight: 1.0
  depth_weight: 0.1
  reg_weight: 0.0001
  
  # Learning rate scheduling
  lr_milestones: [80, 150]
  lr_gamma: 0.5

rendering:
  noise_std: 0.1
  white_bkgd: false
  chunk_size: 1024

output:
  save_dir: "results/lora"
  save_freq: 50
  val_freq: 10
  log_freq: 1

# Expected results
expected_results:
  psnr: 23.3
  ssim: 0.58
  lpips: 0.25
  training_time: "3.2h" 